{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ccd35ff",
   "metadata": {},
   "source": [
    "The contents of this course including lectures, labs, homework assignments, and exams have all been adapted from the [Data 8 course at University California Berkley](https://data.berkeley.edu/education/courses/data-8). Through their generosity and passion for undergraduate education, the Data 8 community at Berkley has opened their content and expertise for other universities to adapt in the name of undergraduate education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datascience\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7ea4d",
   "metadata": {},
   "source": [
    "# Chapter 17: Classification\n",
    "\n",
    "Machine learning is a class of techniques for automatically finding patterns in data and using it to draw inferences or make predictions. You have already seen linear regression, which is one kind of machine learning. This chapter introduces a new one: classification.\n",
    "\n",
    "Classification is about learning how to make predictions from past examples. We are given some examples where we have been told what the correct prediction was, and we want to learn from those examples how to make good predictions in the future.\n",
    "\n",
    "Classification requires data. It involves looking for patterns, and to find patterns, you need data. That’s where the data science comes in. In particular, we’re going to assume that we have access to training data: a bunch of observations, where we know the class of each observation. The collection of these pre-classified observations is also called a training set. A classification algorithm is going to analyze the training set, and then come up with a classifier: an algorithm for predicting the class of future observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79a3e8",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classifyer\n",
    "\n",
    "Let’s work through an example data set that was collected to help doctors diagnose chronic kidney disease (CKD). Each row in the data set represents a single patient who was treated in the past and whose diagnosis is known. For each patient, we have a bunch of measurements from a blood test. We’d like to find which measurements are most useful for diagnosing CKD, and develop a way to classify future patients as “has CKD” or “doesn’t have CKD” based on their blood test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22411494",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd = Table.read_table('ckd.csv').relabeled('Blood Glucose Random', 'Glucose')\n",
    "ckd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137158fa",
   "metadata": {},
   "source": [
    "Some of the variables are categorical (words like “abnormal”), and some quantitative. The quantitative variables all have different scales. We’re going to want to make comparisons and estimate distances, often by eye, so let’s select just a few of the variables and work in standard units. Then we won’t have to worry about the scale of each of the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    return (x - np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94064a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd = Table().with_columns(\n",
    "    'Hemoglobin', standard_units(ckd.column('Hemoglobin')),\n",
    "    'Glucose', standard_units(ckd.column('Glucose')),\n",
    "    'White Blood Cell Count', standard_units(ckd.column('White Blood Cell Count')),\n",
    "    'Class', ckd.column('Class')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0e370",
   "metadata": {},
   "source": [
    "Let’s look at two columns in particular: the hemoglobin level (in the patient’s blood), and the blood glucose level (at a random time in the day; without fasting specially for the blood test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c107450",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_table = Table().with_columns(\n",
    "    'Class', make_array(1, 0),\n",
    "    'Color', make_array('darkblue', 'gold')\n",
    ")\n",
    "ckd = ckd.join('Class', color_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd.scatter('Hemoglobin', 'Glucose', group='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967c2e5",
   "metadata": {},
   "source": [
    "Suppose Alice is a new patient who is not in the data set. If I tell you Alice’s hemoglobin level and blood glucose level, could you predict whether she has CKD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, Alice's Hemoglobin attribute is 0 and her Glucose is 1.5.\n",
    "ckd.scatter('Hemoglobin', 'Glucose', group='Class')\n",
    "plots.scatter(0, 1.5, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d66f8b",
   "metadata": {},
   "source": [
    "If we have Alice’s hemoglobin and glucose numbers, we can put her somewhere on this scatterplot; the hemoglobin is her x-coordinate, and the glucose is her y-coordinate. Now, to predict whether she has CKD or not, we find the nearest point in the scatterplot and check whether it is blue or gold; we predict that Alice should receive the same diagnosis as that patient.\n",
    "\n",
    "This is called ***nearest neighbor classification***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7d015",
   "metadata": {},
   "source": [
    "Thus our nearest neighbor classifier works like this:\n",
    "\n",
    "   * Find the point in the training set that is nearest to the new point.\n",
    "\n",
    "   * If that nearest point is a “CKD” point, classify the new point as “CKD”. If the nearest point is a “not CKD” point, classify the new point as “not CKD”.\n",
    "\n",
    "The scatterplot suggests that this nearest neighbor classifier should be pretty accurate. Points in the lower-right will tend to receive a “no CKD” diagnosis, as their nearest neighbor will be a gold point. The rest of the points will tend to receive a “CKD” diagnosis, as their nearest neighbor will be a blue point. So the nearest neighbor strategy seems to capture our intuition pretty well, for this example.\n",
    "\n",
    "#### Decision Boundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f293a46",
   "metadata": {},
   "source": [
    "<img src=\"Nearest_Neighbors.jpeg\" width=400 height=400 />\n",
    "<img src=\"Nearest_Neighbors_26_0.png\" width=600 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40affb",
   "metadata": {},
   "source": [
    "However, the separation between the two classes won’t always be quite so clean. For instance, suppose that instead of hemoglobin levels we were to look at white blood cell count. Look at what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd766258",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd.scatter('White Blood Cell Count', 'Glucose', group='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bf3e5",
   "metadata": {},
   "source": [
    "If we are given Alice’s glucose level and white blood cell count, can we predict whether she has CKD? Yes, we can make a prediction, but we shouldn’t expect it to be 100% accurate. \n",
    "\n",
    "To improve on *nearest neighbor* we will consider the $k$ nearest neighbor or the *k-nearest neighbor classifier*. To predict Alice’s diagnosis, rather than looking at just the one neighbor closest to her, we can look at the $k$ points that are closest to her, and use the diagnosis for each of those $k$ points to predict Alice’s diagnosis.  We usualy pick $k$ to be an odd number so we do not have to deal with ties.\n",
    "\n",
    "Let's put it to a test.  We will split our data into training and test sets.  We will build a model with the training data and apply the model to the test set to measure the accuracy of the model. \n",
    "\n",
    "Every model should have three groups of individuals:\n",
    "\n",
    "   * a training set on which we can do any amount of exploration to build our classifier;\n",
    "\n",
    "   * a separate testing set on which to try out our classifier and see what fraction of times it classifies correctly;\n",
    "\n",
    "   * the underlying population of individuals for whom we don’t know the true classes; the hope is that our classifier will succeed about as well for these individuals as it did for our testing set.\n",
    "\n",
    "How to generate the training and testing sets? You’ve guessed it – we’ll select at random.\n",
    "\n",
    "There are 158 individuals in ckd. Let’s use a random half of them for training and the other half for testing. To do this, we’ll shuffle all the rows, take the first 79 as the training set, and the remaining 79 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_ckd = ckd.sample(with_replacement=False)\n",
    "training = shuffled_ckd.take(np.arange(79))\n",
    "testing = shuffled_ckd.take(np.arange(79, 158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c17298",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.scatter('White Blood Cell Count', 'Glucose', group='Color')\n",
    "plots.xlim(-2, 6)\n",
    "plots.ylim(-2, 6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701fbfb",
   "metadata": {},
   "source": [
    "### Rows of Tables\n",
    "\n",
    "Until this chapter, we have worked mostly with single columns of tables. But now we have to see whether one individual is “close” to another. Data for individuals are contained in rows of tables.\n",
    "\n",
    "So let’s start by taking a closer look at rows.\n",
    "\n",
    "Here is the original ckd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd = Table.read_table('ckd.csv').relabeled('Blood Glucose Random', 'Glucose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd.row(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2f6a4",
   "metadata": {},
   "source": [
    "Rows are in general **not arrays**, as their elements can be of different types. For example, some of the elements of the row above are strings (like 'abnormal') and some are numerical. So the row can’t be converted into an array.\n",
    "\n",
    "However, rows share some characteristics with arrays. You can use item to access a particular element of a row. For example, to access the Albumin level of Patient 0, we can look at the labels in the printout of the row above to find that it’s item 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd.row(0).item(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4f556",
   "metadata": {},
   "source": [
    "Rows whose elements are all numerical (or all strings) can be converted to arrays. Converting a row to an array gives us access to arithmetic operations and other nice NumPy functions, so it is often useful.\n",
    "\n",
    "Recall we trying to classify the patients as ‘CKD’ or ‘not CKD’, based on two attributes Hemoglobin and Glucose, both measured in standard units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fecb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd = Table().with_columns(\n",
    "    'Hemoglobin', standard_units(ckd.column('Hemoglobin')),\n",
    "    'Glucose', standard_units(ckd.column('Glucose')),\n",
    "    'Class', ckd.column('Class')\n",
    ")\n",
    "\n",
    "color_table = Table().with_columns(\n",
    "    'Class', make_array(1, 0),\n",
    "    'Color', make_array('darkblue', 'gold')\n",
    ")\n",
    "ckd = ckd.join('Class', color_table)\n",
    "ckd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55200332",
   "metadata": {},
   "source": [
    "Here is a scatter plot of the two attributes, along with a red point corresponding to Alice, a new patient. Her value of hemoglobin is 0 (that is, at the average) and glucose 1.1 (that is, 1.1 SDs above average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = make_array(0, 1.1)\n",
    "ckd.scatter('Hemoglobin', 'Glucose', group='Color')\n",
    "plots.scatter(alice.item(0), alice.item(1), color='red', s=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e46542",
   "metadata": {},
   "source": [
    "To find the distance between Alice’s point and any of the other points, we only need the values of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf730c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd_attributes = ckd.select('Hemoglobin', 'Glucose')\n",
    "ckd_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597efdd6",
   "metadata": {},
   "source": [
    "Because the rows now consist only of numerical values, it is possible to convert them to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd_attributes.row(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea032598",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ckd_attributes.row(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c30de",
   "metadata": {},
   "source": [
    "The main calculation we need to do is to find the distance between Alice’s point and any other point. For this, the first thing we need is a way to compute the distance between any pair of points.\n",
    "\n",
    "How do we do this? In 2-dimensional space, it’s pretty easy. If we have a point at coordinates $(x_0,y_0)$\n",
    "and another at $(x_1,y_1)$, the distance between them is $D = \\sqrt{(x_0-x_1)^2 + (y_0-y_1)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient3 = np.array(ckd_attributes.row(3))\n",
    "alice, patient3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.sqrt(np.sum((alice - patient3)**2))\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca217743",
   "metadata": {},
   "source": [
    "Let's wrap this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    \"\"\"Returns the Euclidean distance between point1 and point2.\n",
    "    \n",
    "    Each argument is an array containing the coordinates of a point.\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(alice, patient3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdfc0e",
   "metadata": {},
   "source": [
    "If we want to classify Alice using a k-nearest neighbor classifier, we have to identify her nearest neighbors. What are the steps in this process? Suppose\n",
    "\n",
    ". Then the steps are:\n",
    "\n",
    "   * Step 1. Find the distance between Alice and each point in the training sample.\n",
    "\n",
    "   * Step 2. Sort the data table in increasing order of the distances.\n",
    "\n",
    "   * Step 3. Take the top 5 rows of the sorted table.\n",
    "\n",
    "What we need is a function that finds the distance between Alice and another point whose coordinates are contained in a row. The function distance returns the distance between any two points whose coordinates are in arrays. We can use that to define distance_from_alice, which takes a row as its argument and returns the distance between that row and Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_alice(row):\n",
    "    \"\"\"Returns distance between Alice and a row of the attributes table\"\"\"\n",
    "    return distance(alice, np.array(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d88cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_from_alice(ckd_attributes.row(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd9646",
   "metadata": {},
   "source": [
    "Recall that if you want to apply a function to each element of a column of a table, one way to do that is by the call *table_name.apply(function_name, column_label)*. This evaluates to an array consisting of the values of the function when we call it on each element of the column. So each entry of the array is based on the corresponding row of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd_attributes.apply(distance_from_alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde87d86",
   "metadata": {},
   "source": [
    "Let's put it into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d747bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckd_with_distances = ckd.with_column('Distance from Alice', ckd_attributes.apply(distance_from_alice))\n",
    "ckd_with_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cb597",
   "metadata": {},
   "source": [
    "For Step 2, let’s sort the table in increasing order of distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_distance = ckd_with_distances.sort('Distance from Alice')\n",
    "sorted_by_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36561f57",
   "metadata": {},
   "source": [
    "Step 3: The top 5 rows correspond to Alice’s 5 nearest neighbors; you can replace 5 by any other positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6777d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_5_nearest_neighbors = sorted_by_distance.take(np.arange(5))\n",
    "alice_5_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fe93e",
   "metadata": {},
   "source": [
    "Three of Alice’s five nearest neighbors are blue points and two are gold. So a 5-nearest neighbor classifier would classify Alice as blue: it would predict that Alice has chronic kidney disease.\n",
    "\n",
    "The graph below zooms in on Alice and her five nearest neighbors. The two gold ones just inside the circle directly below the red point. The classifier says Alice is more like the three blue ones around her."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02632a0",
   "metadata": {},
   "source": [
    "<img src=\"Rows_of_Tables_49_0.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c48fa",
   "metadata": {},
   "source": [
    "### Banknote authentication example\n",
    "\n",
    "This time we’ll look at predicting whether a banknote (e.g., a $20 bill) is counterfeit or legitimate. Researchers have put together a data set for us, based on photographs of many individual banknotes: some counterfeit, some legitimate. For each banknote, we know a few numbers that were computed from a photograph of it as well as its class (whether it is counterfeit or not). Let’s load it into a table and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e3cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "banknotes = Table.read_table('banknote.csv')\n",
    "banknotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_table = Table().with_columns(\n",
    "    'Class', make_array(1, 0),\n",
    "    'Color', make_array('darkblue', 'gold')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "banknotes = banknotes.join('Class', color_table)\n",
    "banknotes.scatter('WaveletVar', 'WaveletCurt', group='Color')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6696e1",
   "metadata": {},
   "source": [
    "hose two measurements do seem helpful for predicting whether the banknote is counterfeit or not. However, in this example you can now see that there is some overlap between the blue cluster and the gold cluster. This indicates that there will be some images where it’s hard to tell whether the banknote is legitimate based on just these two numbers. Still, you could use a k-nearest neighbor classifier to predict the legitimacy of a banknote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f845fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "banknotes.scatter('WaveletSkew', 'Entropy', group='Color')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97a9c6",
   "metadata": {},
   "source": [
    "There does seem to be a pattern, but it’s a pretty complex one. Nonetheless, the k-nearest neighbors classifier can still be used and will effectively “discover” patterns out of this. \n",
    "\n",
    "So far I’ve been assuming that we have exactly 2 attributes that we can use to help us make our prediction. What if we have more than 2? For instance, what if we have 3 attributes?\n",
    "\n",
    "There’s nothing special about 2 or 3. If you have 4 attributes, you can use the k-nearest neighbors classifier in 4 dimensions. 5 attributes? Work in 5-dimensional space. And no need to stop there! This all works for arbitrarily many attributes; you just work in a very high dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plots.figure(figsize=(8,8)).add_subplot(111, projection='3d')\n",
    "ax.scatter(banknotes.column('WaveletSkew'), \n",
    "           banknotes.column('WaveletVar'), \n",
    "           banknotes.column('WaveletCurt'), \n",
    "           c=banknotes.column('Color'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb2019",
   "metadata": {},
   "source": [
    "When we use these 3 attributes, the two clusters have almost no overlap. In other words, a classifier that uses these 3 attributes will be more accurate than one that only uses the 2 attributes.\n",
    "\n",
    "### Distance in multiple dimentions\n",
    "\n",
    "In 3-dimensional space, the points are $(x_0, y_0, z_0)$ and $(x_1, y_1, z_1)$ , and the formula for the distance between them is:\n",
    "\n",
    "$D = \\sqrt{(x_0-x_1)^2 + (y_0-y_1)^2 + (z_0-z_1)^2}$\n",
    "\n",
    "In $n$-dimensional space, things are a bit harder to visualize but the equation follows the same pattern.\n",
    "\n",
    "Let's look at a new example. The table [wine](https://archive.ics.uci.edu/ml/datasets/Wine) contains the chemical composition of 178 different Italian wines. The classes are the grape species, called cultivars. There are three classes but let’s just see whether we can tell Class 1 apart from the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8203b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = Table.read_table('wine.csv')\n",
    "\n",
    "# For converting Class to binary\n",
    "\n",
    "def is_one(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "wine = wine.with_column('Class', wine.apply(is_one, 0))\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a12404",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_attributes = wine.drop('Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e8ccb",
   "metadata": {},
   "source": [
    "The first two wines are both in Class 1. To find the distance between them, we first need a table of just the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(np.array(wine_attributes.row(0)), np.array(wine_attributes.row(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fff1f",
   "metadata": {},
   "source": [
    "The last wine in the table is of Class 0. Its distance from the first wine is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(np.array(wine_attributes.row(0)), np.array(wine_attributes.row(177)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_with_colors = wine.join('Class', color_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_with_colors.scatter('Flavanoids', 'Alcohol', group='Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_with_colors.scatter('Alcalinity of Ash', 'Ash', group='Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_with_colors.scatter('Magnesium', 'Total Phenols', group='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595c0b7",
   "metadata": {},
   "source": [
    "### Let's build a classifier\n",
    "\n",
    "The input is a point that we want to classify. The classifier works by finding the $k$ nearest neighbors of point from the training set. So, our approach will go like this:\n",
    "\n",
    "   * Find the closest $k$ neighbors of point, i.e., the $k$ wines from the training set that are most similar to point.\n",
    "   * Look at the classes of those $k$ neighbors, and take the majority vote to find the most-common class of wine. Use that as our predicted class for point.\n",
    "   \n",
    "To implement the first step for the kidney disease data, we had to compute the distance from each patient in the training set to point, sort them by distance, and take the $k$ closest patients in the training set.\n",
    "\n",
    "That’s what we did in the previous section with the point corresponding to Alice. Let’s generalize that code. We’ll redefine distance here, just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    \"\"\"Returns the distance between point1 and point2\n",
    "    where each argument is an array \n",
    "    consisting of the coordinates of the point\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def all_distances(training, new_point):\n",
    "    \"\"\"Returns an array of distances\n",
    "    between each point in the training set\n",
    "    and the new point (which is a row of attributes)\"\"\"\n",
    "    attributes = training.drop('Class')\n",
    "    def distance_from_point(row):\n",
    "        return distance(np.array(new_point), np.array(row))\n",
    "    return attributes.apply(distance_from_point)\n",
    "\n",
    "def table_with_distances(training, new_point):\n",
    "    \"\"\"Augments the training table \n",
    "    with a column of distances from new_point\"\"\"\n",
    "    return training.with_column('Distance', all_distances(training, new_point))\n",
    "\n",
    "def closest(training, new_point, k):\n",
    "    \"\"\"Returns a table of the k rows of the augmented table\n",
    "    corresponding to the k smallest distances\"\"\"\n",
    "    with_dists = table_with_distances(training, new_point)\n",
    "    sorted_by_distance = with_dists.sort('Distance')\n",
    "    topk = sorted_by_distance.take(np.arange(k))\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_wine = wine.drop('Class').row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96436d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest(wine, special_wine, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac50ed",
   "metadata": {},
   "source": [
    "Next we need to take a “majority vote” of the nearest neighbors and assign our point the same class as the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce65af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(topkclasses):\n",
    "    return topkclasses.group('Class').sort('count').column('Class').item(0)\n",
    "\n",
    "def classify(training, new_point, k):\n",
    "    closestk = closest(training, new_point, k)\n",
    "    topkclasses = closestk.select('Class')\n",
    "    return majority(topkclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ff0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(wine, special_wine, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78574be",
   "metadata": {},
   "source": [
    "If we change special_wine to be the last one in the dataset, is our classifier able to tell that it’s in Class 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_wine = wine.drop('Class').row(177)\n",
    "classify(wine, special_wine, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add9684",
   "metadata": {},
   "source": [
    "But we don’t yet know how it does with all the other wines, and in any case we know that testing on wines that are already part of the training set might be over-optimistic. We will split the data into training and test sets and measure the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08120509",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_wine = wine.sample(with_replacement=False) \n",
    "training_set = shuffled_wine.take(np.arange(89))\n",
    "test_set  = shuffled_wine.take(np.arange(89, 178))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(training, test, k):\n",
    "    test_attributes = test.drop('Class')\n",
    "    def classify_testrow(row):\n",
    "        return classify(training, row, k)\n",
    "    c = test_attributes.apply(classify_testrow)\n",
    "    return (test.num_rows - (np.count_nonzero(c - test_set.column('Class')))) / test.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32eddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(training_set, test_set, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35033d2",
   "metadata": {},
   "source": [
    "The accuracy rate isn’t bad at all for a simple classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45acc9",
   "metadata": {},
   "source": [
    "# Chapter 18: Updating Predictions\n",
    "\n",
    "\n",
    "Suppose that we eventually find out the true class of our new point. Then we will know whether we got the classification right. Also, we will have a new point that we can add to our training set, because we know its class. This updates our training set. So, naturally, we will want to update our classifier based on the new training set.\n",
    "\n",
    "This chapter looks at some simple scenarios where new data leads us to update our predictions. While the examples in the chapter are simple in terms of calculation, the method of updating can be generalized to work in complex settings and is one of the most powerful tools used for machine learning.\n",
    "\n",
    "\n",
    "\n",
    "Let’s try to use data to classify a point into one of two categories, choosing the category that we think is more likely than not. To do this, we not only need the data but also a clear description of how chances are involved.\n",
    "\n",
    "We will start out in a simple artifical setting just to develop the main technique, and then move to a more intriguing example.\n",
    "\n",
    "Suppose there is a university class with the following composition:\n",
    "\n",
    "   * 60% of the students are Second Years and the remaining 40% are Third Years\n",
    "\n",
    "   * 50% of the Second Years have declared their major\n",
    "\n",
    "   * 80% of the Third Years have declared their major\n",
    "\n",
    "Now suppose I pick a student at random from the class. Can you classify the student as Second Year or Third Year, using our “more likely than not” criterion?\n",
    "\n",
    "You can, because the student is picked at random and so you know that the chance that the student is a Second Year is 60%. That’s greater than the 40% chance of being a Third Year, so you would classify the student as Second Year.\n",
    "\n",
    "The information about the majors is irrelevant, as we already know the proportions of Second and Third Years in the class.\n",
    "\n",
    "We have a pretty simple classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37610490",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = Table().with_columns('Year', np.concatenate((np.full(60, \"Second\"), np.full(40, \"Third\"))),\n",
    "                               'Major', np.concatenate((np.full(30, \"Declared\"), np.full(30, \"Undeclared\"), np.full(32, \"Declared\"), np.full(8, \"Undeclared\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f09168",
   "metadata": {},
   "outputs": [],
   "source": [
    "students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf297c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "students.pivot('Major', 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8670ed",
   "metadata": {},
   "source": [
    "But now suppose I give you some additional information about the student who was picked:\n",
    "\n",
    "The student has declared a major.\n",
    "\n",
    "Would this knowledge change your classification?\n",
    "\n",
    "Now it becomes important to look at the relation between year and major declaration. It’s still true that more students are Second Years than Third Years. But it’s also true that among the Third Years, a much higher percent have declared their major than among the Second Years. Our classifier has to take both of these observations into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef2a1a",
   "metadata": {},
   "source": [
    "Now the student can only be in one of the two Declared cells.\n",
    "\n",
    "There are 62 students in those cells, and 32 out of the 62 are Third Years. That’s more than half, even though not by much.\n",
    "\n",
    "So, in the light of the new information about the student’s major, we have to update our prediction and now classify the student as a Third Year.\n",
    "\n",
    "What is the chance that our classification is correct? We will be right for all the 32 Third Years who are Declared, and wrong for the 30 Second Years who are Declared. The chance that we are correct is therefore about 0.516.\n",
    "\n",
    "In other words, the chance that we are correct is the proportion of Third Years among the students who have Declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "32/(30+32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "students.pivot('Major', 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc02cad",
   "metadata": {},
   "source": [
    "<img src=\"tree_students.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fbaa5",
   "metadata": {},
   "source": [
    "Like the pivot table, this diagram partitions the students into four distinct groups known as “branches”. Notice that the “Third Year, Declared” branch contains the proportion 0.4 x 0.8 = 0.32 of the students, corresponding to the 32 students in the “Third Year, Declared” cell of the pivot table. The “Second Year, Declared” branch contains 0.6 x 0.5 = 0.3 of the students, corresponding to the 30 in the “Second Year, Declared” cell of the pivot table.\n",
    "\n",
    "We know that the student who was picked belongs to a “Declared” branch; that is, the student is either in the top branch or the third from top. Those two branches now form our reduced space of possibilities, and all chances have to be calculated relative to the total chance of this reduced space.\n",
    "\n",
    "So, given that the student is Declared, the chance of them being a Third Year can be calculated directly from the tree. The answer is the proportion in the “Third Year, Declared” branch relative to the total proportion in the two “Declared” branches.\n",
    "\n",
    "That is, the answer is the proportion of Third Years among students who are Declared, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a559e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.4 * 0.8)/(0.6 * 0.5  +  0.4 * 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed569bb",
   "metadata": {},
   "source": [
    "The method that we have just used is due to the Reverend [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) (1701-1761). His method solved what was called an “inverse probability” problem: given new data, how can you update chances you had found earlier? Though Bayes lived three centuries ago, his method is [widely used](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) now in machine learning.\n",
    "\n",
    "We will state the rule in the context of our population of students. First, some terminology:\n",
    "\n",
    "**Prior probabilities**. Before we knew the chosen student’s major declaration status, the chance that the student was a Second Year was 60% and the chance that the student was a Third Year was 40%. These are the prior probabilities of the two categories.\n",
    "\n",
    "**Likelihoods**. These are the chances of the Major status, given the category of student; thus they can be read off the tree diagram. For example, the likelihood of Declared status given that the student is a Second Year is 0.5.\n",
    "\n",
    "**Posterior probabilities**. These are the chances of the two Year categories, after we have taken into account information about the Major declaration status. We computed one of these:\n",
    "\n",
    "The posterior probability that the student is a Third Year, given that the student has Declared, is denoted $P(\\text{Third Year} ~\\big{\\vert}~ \\text{Declared})$\n",
    "and is calculated as follows:\n",
    "\n",
    "$\\begin{split}\n",
    "\\begin{align*}\n",
    "P(\\mbox{Third Year} ~\\big{\\vert}~ \\mbox{Declared}) \n",
    "~ &=~ \\frac{ 0.4 \\times 0.8}{0.6 \\times 0.5 ~+~ 0.4 \\times  0.8} \\\\ \\\\\n",
    "&=~ \\frac{\\mbox{(prior probability of Third Year)} \\times\n",
    "\\mbox{(likelihood of Declared given Third Year)}}\n",
    "{\\mbox{total probability of Declared}}\n",
    "\\end{align*}\n",
    "\\end{split}$\n",
    "\n",
    "The other posterior probability is:\n",
    "\n",
    "$\\begin{split}\n",
    "\\begin{align*}\n",
    "P(\\mbox{Second Year} ~\\big{\\vert}~ \\mbox{Declared})\n",
    "~ &=~ \\frac{ 0.6 \\times 0.5}{0.6 \\times 0.5 ~+~ 0.4 \\times  0.8} \\\\ \\\\\n",
    "&=~ \\frac{\\mbox{(prior probability of Second Year)} \\times\n",
    "\\mbox{(likelihood of Declared given Second Year)}}\n",
    "{\\mbox{total probability of Declared}}\n",
    "\\end{align*}\n",
    "\\end{split}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.6 * 0.5)/(0.6 * 0.5  +  0.4 * 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bae47a",
   "metadata": {},
   "source": [
    "Notice that both the posterior probabilities have the same denominator: the chance of the new information, which is that the student has Declared.\n",
    "\n",
    "Because of this, Bayes’ method is sometimes summarized as a statement about proportionality: \n",
    "\n",
    "$\\mbox{posterior} ~ \\propto ~ \\mbox{prior} \\times \\mbox{likelihood}$\n",
    "\n",
    "\n",
    "Formulas are great for efficiently describing calculations. But in settings like our example about students, it is simpler not to think in terms of formulas. Just use the tree diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36992b",
   "metadata": {},
   "source": [
    "### Medical Test Example\n",
    "\n",
    "Many medical tests for diseases return Positive or Negative results.\n",
    "\n",
    "Medical tests are carefully designed to be very accurate. But few tests are accurate 100% of the time. Almost all tests make errors of two kinds:\n",
    "\n",
    "   * A false positive is an error in which the test concludes Positive but the patient doesn’t have the disease.\n",
    "\n",
    "   * A false negative is an error in which the test concludes Negative but the patient does have the disease.\n",
    "\n",
    "These errors can affect people’s decisions. False positives can cause anxiety and unnecessary treatment (which in some cases is expensive or dangerous). False negatives can have even more serious consequences if the patient doesn’t receive treatment because of their Negative test result.\n",
    "\n",
    "\n",
    "Suppose there is a large population and a disease that strikes a tiny proportion of the population. The tree diagram below summarizes information about such a disease and about a medical test for it.\n",
    "\n",
    "<img src=\"tree_disease_rare.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9616c7f",
   "metadata": {},
   "source": [
    "So suppose a person is picked at random from the population and tested. If the test result is Positive, how would you classify them: Disease, or No disease?\n",
    "\n",
    "We can answer this by applying Bayes’ Rule and using our “more likely than not” classifier. Given that the person has tested Positive, the chance that he or she has the disease is the proportion in the top branch, relative to the total proportion in the Test Positive branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.004 * 0.99)/(0.004 * 0.99  +  0.996*0.005 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea39364",
   "metadata": {},
   "source": [
    "The chance they have the disease is 44%!  This is a strange conclusion. We have a pretty accurate test, and a person who has tested Positive, and our classification is … that they don’t have the disease? That doesn’t seem to make any sense.  \n",
    "\n",
    "However, keep in mind that this is the result for the entire population and you are more likely to have a falst positive than than a true positive.\n",
    "\n",
    "The tiny fraction of those that falsely test Positive are still greater in number than the people who correctly test Positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72db232",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = Table().with_columns('True Condition', np.concatenate((np.full(400, \"Disease\"), np.full(99600, \"No Disease\"))),\n",
    "                               'Test Result', np.concatenate((np.full(396, \"Positive\"), np.full(4, \"Negative\"), np.full(99102, \"Negative\"), np.full(498, \"Positive\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fffa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease.pivot('Test Result', 'True Condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "396/(396 + 498)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f834a",
   "metadata": {},
   "source": [
    "Suppose the doctor’s subjective opinion is that there is a 5% chance that the patient has the disease. Then just the prior probabilities in the tree diagram will change:\n",
    "\n",
    "<img src=\"tree_disease_subj.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.05 * 0.99)/(0.05 * 0.99  +  0.95 * 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a93dca",
   "metadata": {},
   "source": [
    "Even though the doctor has a pretty low prior probability (5%) that the patient has the disease, once the patient tests Positive the posterior probability of having the disease shoots up to more than 91%.\n",
    "\n",
    "If the patient tests Positive, it would be reasonable for the doctor to proceed as though the patient has the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_subj = Table().with_columns('True Condition', np.concatenate((np.full(5000, \"Disease\"), np.full(95000, \"No Disease\"))),\n",
    "                               'Test Result', np.concatenate((np.full(4950, \"Positive\"), np.full(50, \"Negative\"), np.full(94525, \"Negative\"), np.full(475, \"Positive\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_subj.pivot('Test Result', 'True Condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "4950/(4950 + 475)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2850bd9",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d1f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
