{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ccd35ff",
      "metadata": {
        "id": "3ccd35ff"
      },
      "source": [
        "The contents of this course including lectures, labs, homework assignments, and exams have all been adapted from the [Data 8 course at University California Berkley](https://data.berkeley.edu/education/courses/data-8). Through their generosity and passion for undergraduate education, the Data 8 community at Berkley has opened their content and expertise for other universities to adapt in the name of undergraduate education."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1c4110",
      "metadata": {
        "id": "cb1c4110"
      },
      "outputs": [],
      "source": [
        "!pip install datascience -q\n",
        "from datascience import *\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plots\n",
        "plots.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mrsnellek/DS410.git"
      ],
      "metadata": {
        "id": "7TlwEeSN5Crt"
      },
      "id": "7TlwEeSN5Crt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd DS410/Week_4/Week_4_Lec/"
      ],
      "metadata": {
        "id": "9VcJvTMl5IDJ"
      },
      "id": "9VcJvTMl5IDJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f7c7ea4d",
      "metadata": {
        "id": "f7c7ea4d"
      },
      "source": [
        "# Chapter 14: Why the Mean Matters\n",
        "\n",
        "In this chapter, we will study means and what we can say about them with only minimal assumptions about the underlying populations. Question that we will address include:\n",
        "\n",
        "   * What exactly does the mean measure?\n",
        "\n",
        "   * How close to the mean are most of the data?\n",
        "\n",
        "   * How is the sample size related to the variability of the sample mean?\n",
        "\n",
        "   * Why do empirical distributions of random sample means come out bell shaped?\n",
        "\n",
        "   * How can we use sample means effectively for inference?\n",
        "\n",
        "The average or mean of a collection of numbers is the sum of all the elements of the collection, divided by the number of elements in the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4cf5638",
      "metadata": {
        "id": "a4cf5638"
      },
      "outputs": [],
      "source": [
        "not_symmetric = make_array(2, 3, 3, 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea66920",
      "metadata": {
        "id": "7ea66920"
      },
      "outputs": [],
      "source": [
        "np.average(not_symmetric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65cbaa9",
      "metadata": {
        "id": "d65cbaa9"
      },
      "outputs": [],
      "source": [
        "np.mean(not_symmetric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8bb28d",
      "metadata": {
        "id": "ad8bb28d"
      },
      "outputs": [],
      "source": [
        "np.mean(make_array(1, 1, 1, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4f9e30",
      "metadata": {
        "id": "9b4f9e30"
      },
      "outputs": [],
      "source": [
        "np.mean(make_array(True, True, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bb6184",
      "metadata": {
        "id": "66bb6184"
      },
      "source": [
        "The mean of the collection {2, 3, 3, 9} is 4.25, which is not the “halfway point” of the data. So then what does the mean measure?\n",
        "\n",
        "To see this, notice that the mean can be calculated in different ways.\n",
        "\n",
        "\n",
        "\\begin{split}\\begin{align*}\n",
        "\\mbox{mean} ~ &=~ 4.25 \\\\ \\\\\n",
        "&=~ \\frac{2 + 3 + 3 + 9}{4} \\\\ \\\\\n",
        "&=~ 2 \\cdot \\frac{1}{4} ~~ + ~~ 3 \\cdot \\frac{1}{4} ~~ + ~~ 3 \\cdot \\frac{1}{4} ~~ + ~~ 9 \\cdot \\frac{1}{4} \\\\ \\\\\n",
        "&=~ 2 \\cdot \\frac{1}{4} ~~ + ~~ 3 \\cdot \\frac{2}{4} ~~ + ~~ 9 \\cdot \\frac{1}{4} \\\\ \\\\\n",
        "&=~ 2 \\cdot 0.25 ~~ + ~~ 3 \\cdot 0.5 ~~ + ~~ 9 \\cdot 0.25\n",
        "\\end{align*}\\end{split}\n",
        "\n",
        "* When we calculate the mean, each distinct value in the collection is weighted by the proportion of times it appears in the collection.\n",
        "* The mean of a collection depends only on the distinct values and their proportions. \n",
        "* If two collections have the same distribution, then they have the same mean.\n",
        "\n",
        "For example, here is another collection that has the same distribution as not_symmetric and hence the same mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106f66fc",
      "metadata": {
        "id": "106f66fc"
      },
      "outputs": [],
      "source": [
        "not_symmetric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce73cd40",
      "metadata": {
        "id": "ce73cd40"
      },
      "outputs": [],
      "source": [
        "same_distribution = make_array(2, 2, 3, 3, 3, 3, 9, 9)\n",
        "np.mean(same_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67b45f0",
      "metadata": {
        "id": "d67b45f0"
      },
      "source": [
        "The mean is the center of gravity or balance point of the histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0982487e",
      "metadata": {
        "id": "0982487e"
      },
      "outputs": [],
      "source": [
        "Table().with_column(\"Dist\", same_distribution).hist()\n",
        "plots.scatter(np.mean(same_distribution), -0.02, color='red', s=100, zorder=10, marker = \"^\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c46799",
      "metadata": {
        "id": "b8c46799"
      },
      "source": [
        "The table fc2020 contains salary and benefits data for Fort Collins City employees in 2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562b7d31",
      "metadata": {
        "id": "562b7d31"
      },
      "outputs": [],
      "source": [
        "fc2020 = Table.read_table('2020_Fort_Collins_City_Employee_Base_Pay.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a100a3",
      "metadata": {
        "id": "08a100a3"
      },
      "outputs": [],
      "source": [
        "fc_bins = np.arange(0, 300000, 10000)\n",
        "fc2020.select('Annual Salary').hist(bins=fc_bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82bd5973",
      "metadata": {
        "id": "82bd5973"
      },
      "source": [
        "This histogram is skewed to the right; it has a right-hand tail.\n",
        "\n",
        "\n",
        "In general, **if the histogram has a tail on one side (the formal term is “skewed”), then the mean is pulled away from the median in the direction of the tail.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b009b45",
      "metadata": {
        "id": "0b009b45"
      },
      "outputs": [],
      "source": [
        "compensation = fc2020.column('Annual Salary')\n",
        "percentile(50, compensation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.median(compensation)"
      ],
      "metadata": {
        "id": "qMhx57MY5gtD"
      },
      "id": "qMhx57MY5gtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36942c15",
      "metadata": {
        "id": "36942c15"
      },
      "outputs": [],
      "source": [
        "np.mean(compensation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c7a79a",
      "metadata": {
        "id": "a4c7a79a"
      },
      "source": [
        "Distributions of incomes of large populations tend to be right skewed. When the bulk of a population has middle to low incomes, but a very small proportion has very high incomes, the histogram has a long, thin tail to the right.\n",
        "\n",
        "That is why economists often summarize income distributions by the median instead of the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df2c3e5",
      "metadata": {
        "id": "9df2c3e5"
      },
      "source": [
        "### Variability\n",
        "\n",
        "The mean tells us where a histogram balances. But in most every histograms values spread out on both sides of the mean. How far from the mean can they be? To answer this question, we will develop a measure of variability about the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f21b81",
      "metadata": {
        "id": "61f21b81"
      },
      "outputs": [],
      "source": [
        "any_numbers = make_array(1, 2, 2, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42b084b",
      "metadata": {
        "id": "a42b084b"
      },
      "source": [
        "The goal is to measure roughly how far off the numbers are from their average. To do this, we first need the average:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f421b488",
      "metadata": {
        "id": "f421b488"
      },
      "outputs": [],
      "source": [
        "# Step 1. The average.\n",
        "mean = np.mean(any_numbers)\n",
        "mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b667214c",
      "metadata": {
        "id": "b667214c"
      },
      "outputs": [],
      "source": [
        "# Step 2. The deviations from average.\n",
        "deviations = any_numbers - mean\n",
        "calculation_steps = Table().with_columns(\n",
        "        'Value', any_numbers,\n",
        "        'Deviation from Average', deviations\n",
        "        )\n",
        "calculation_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a594ee14",
      "metadata": {
        "id": "a594ee14"
      },
      "source": [
        "To calculate roughly how big the deviations are, it is natural to compute the mean of the deviations. But something interesting happens when all the deviations are added together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "704afd1b",
      "metadata": {
        "id": "704afd1b"
      },
      "outputs": [],
      "source": [
        "sum(deviations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51c09be",
      "metadata": {
        "id": "e51c09be"
      },
      "source": [
        "**The sum of the deviations from average is zero.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455c3717",
      "metadata": {
        "id": "455c3717"
      },
      "source": [
        "What we really want to know is roughly how big the deviations are, regardless of whether they are positive or negative. So we need a way to eliminate the signs of the deviations.\n",
        "\n",
        "There are two time-honored ways of losing signs: the absolute value, and the square. It turns out that taking the square is better because it places a larger emphasis on outliers and values that deviate more from the mean.\n",
        "\n",
        "So let’s eliminate the signs by squaring all the deviations. Then we will take the mean of the squares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a054dfb6",
      "metadata": {
        "id": "a054dfb6"
      },
      "outputs": [],
      "source": [
        "# Step 3. The squared deviations from average\n",
        "squared_deviations = deviations ** 2\n",
        "calculation_steps = calculation_steps.with_column(\n",
        "   'Squared Deviations from Average', squared_deviations\n",
        "    )\n",
        "calculation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e7fab4f",
      "metadata": {
        "id": "6e7fab4f"
      },
      "outputs": [],
      "source": [
        "# Step 4. Variance = the mean squared deviation from average\n",
        "\n",
        "variance = np.mean(squared_deviations)\n",
        "variance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7abc2bca",
      "metadata": {
        "id": "7abc2bca"
      },
      "source": [
        "**Variance: The mean squared deviation calculated above is called the variance of the values.**\n",
        "\n",
        "While the variance does give us an idea of spread, it is not on the same scale as the original variable as its units are the square of the original. This makes interpretation very difficult.\n",
        "\n",
        "So we return to the original scale by taking the positive square root of the variance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577a9e53",
      "metadata": {
        "id": "577a9e53"
      },
      "outputs": [],
      "source": [
        "# Step 5.\n",
        "# Standard Deviation:    root mean squared deviation from average\n",
        "# Steps of calculation:   5    4      3       2             1\n",
        "sd = variance ** 0.5\n",
        "sd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612962e9",
      "metadata": {
        "id": "612962e9"
      },
      "source": [
        "The quantity that we have just computed is called the standard deviation of the list, and is abbreviated as SD. It measures roughly how far the numbers on the list are from their average.\n",
        "\n",
        "**Definition. The SD of a list is defined as the root mean square of deviations from average.** That’s a mouthful. But read it from right to left and you have the sequence of steps in the calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a87486",
      "metadata": {
        "id": "f7a87486"
      },
      "outputs": [],
      "source": [
        "np.std(any_numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b11a9a",
      "metadata": {
        "id": "70b11a9a"
      },
      "source": [
        "\n",
        "\n",
        "The table nba13 contains data on the players in the National Basketball Association (NBA) in 2013. For each player, the table records the position at which the player usually played, his height in inches, his weight in pounds, and his age in years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b5c404",
      "metadata": {
        "id": "e7b5c404"
      },
      "outputs": [],
      "source": [
        "nba13 = Table.read_table('nba2013.csv')\n",
        "nba13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82b0e64",
      "metadata": {
        "id": "e82b0e64"
      },
      "outputs": [],
      "source": [
        "nba13.select('Height').hist(bins=np.arange(68, 88, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "951ee30b",
      "metadata": {
        "id": "951ee30b"
      },
      "outputs": [],
      "source": [
        "mean_height = np.mean(nba13.column('Height'))\n",
        "mean_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4731eee7",
      "metadata": {
        "id": "4731eee7"
      },
      "outputs": [],
      "source": [
        "mean_height/12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc8459d",
      "metadata": {
        "id": "7bc8459d"
      },
      "source": [
        "About how far off are the players’ heights from the average? This is measured by the SD of the heights, which is about 3.45 inches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121e33cf",
      "metadata": {
        "id": "121e33cf"
      },
      "outputs": [],
      "source": [
        "sd_height = np.std(nba13.column('Height'))\n",
        "sd_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a7ff40",
      "metadata": {
        "id": "e8a7ff40"
      },
      "outputs": [],
      "source": [
        "nba13.sort('Height', descending=True).show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ffc0bc",
      "metadata": {
        "id": "38ffc0bc"
      },
      "source": [
        "Thabeet was tallest player and about 8 inches above the average height."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a51178",
      "metadata": {
        "id": "94a51178"
      },
      "outputs": [],
      "source": [
        "87 - mean_height"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94845483",
      "metadata": {
        "id": "94845483"
      },
      "source": [
        "Which is about 2.3 SD from the mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d4578c",
      "metadata": {
        "id": "70d4578c"
      },
      "outputs": [],
      "source": [
        "(87 - mean_height)/sd_height"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1339f79",
      "metadata": {
        "id": "c1339f79"
      },
      "source": [
        "Isaiah Thomas was one of the two shortest NBA players in 2013. His height was about 2.9 SDs below average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005d7252",
      "metadata": {
        "id": "005d7252"
      },
      "outputs": [],
      "source": [
        "nba13.sort('Height').show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c210a36",
      "metadata": {
        "id": "6c210a36"
      },
      "outputs": [],
      "source": [
        "(69 - mean_height)/sd_height"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbccdaa",
      "metadata": {
        "id": "4fbccdaa"
      },
      "source": [
        "What we have observed is that the tallest and shortest players were both just a few SDs away from the average height. This is an example of why the SD is a useful measure of spread. No matter what the shape of the histogram, the average and the SD together tell you a lot about where the histogram is situated on the number line."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50971f2",
      "metadata": {
        "id": "a50971f2"
      },
      "source": [
        "The Russian mathematician [Pafnuty Chebychev](https://en.wikipedia.org/wiki/Pafnuty_Chebyshev) (1821-1894) proved a result that makes our rough statements precise.\n",
        "\n",
        "For all lists, and all numbers the proportion of entries that are in the range “average SDs” is at least  \n",
        "\n",
        "\n",
        "1 - 1/z^2\n",
        "\n",
        "What makes the result powerful is that it is true for all lists – all distributions, no matter how irregular.\n",
        "\n",
        "Specifically, it says that for every list:\n",
        "\n",
        "* the proportion in the range “average 2 SDs” is at least 1 - 1/4 = 0.75\n",
        "\n",
        "* the proportion in the range “average 3 SDs” is at least 1 - 1/9 ~ 0.89\n",
        "\n",
        "* the proportion in the range “average 4.5 SDs” is at least 1 - 1/4.5^2 ~ 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79433bd9",
      "metadata": {
        "id": "79433bd9"
      },
      "source": [
        "### Standard Units"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b1611c",
      "metadata": {
        "id": "30b1611c"
      },
      "source": [
        "In the calculations above, ***z*** measures standard units, the number of standard deviations above average.\n",
        "\n",
        "No matter what the distribution of the list looks like, Chebychev’s bounds imply that standard units will typically be in the (-5, 5) range.\n",
        "\n",
        "z = (value - average) / SD\n",
        "\n",
        "As we will see, standard units are frequently used in data analysis. So it is useful to define a function that converts an array of numbers to standard units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5a994a",
      "metadata": {
        "id": "4a5a994a"
      },
      "outputs": [],
      "source": [
        "def standard_units(numbers_array):\n",
        "    \"Convert any array of numbers to standard units.\"\n",
        "    return (numbers_array - np.mean(numbers_array))/np.std(numbers_array)    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cced379",
      "metadata": {
        "id": "8cced379"
      },
      "source": [
        "### Example United Flights\n",
        "As we saw in an earlier section, the table united contains a column Delay consisting of the departure delay times, in minutes, of over thousands of United Airlines flights in the summer of 2015. We will create a new column called Delay (Standard Units) by applying the function standard_units to the column of delay times. This allows us to see all the delay times in minutes as well as their corresponding values in standard units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ed9f80",
      "metadata": {
        "id": "e0ed9f80"
      },
      "outputs": [],
      "source": [
        "united = Table.read_table('united.csv')\n",
        "united = united.with_column(\n",
        "    'Delay (Standard Units)', standard_units(united.column('Delay'))\n",
        ")\n",
        "united"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ce98c8",
      "metadata": {
        "id": "f7ce98c8"
      },
      "outputs": [],
      "source": [
        "united.sort('Delay', descending=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ec93aa",
      "metadata": {
        "id": "24ec93aa"
      },
      "source": [
        "ut something rather alarming happens when we sort the delay times from highest to lowest. The standard units that we can see are extremely high!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d063337",
      "metadata": {
        "id": "9d063337"
      },
      "source": [
        "What this shows is that it is possible for data to be many SDs above average (and for flights to be delayed by almost 10 hours). The highest value of delay is more than 14 in standard units."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf7d041",
      "metadata": {
        "id": "4cf7d041"
      },
      "source": [
        "However, the proportion of these extreme values is small, and Chebychev’s bounds still hold true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89cecfb",
      "metadata": {
        "id": "c89cecfb"
      },
      "outputs": [],
      "source": [
        "within_3_sd = united.where('Delay (Standard Units)', are.between(-3, 3))\n",
        "within_3_sd.num_rows/united.num_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "212e25de",
      "metadata": {
        "id": "212e25de"
      },
      "source": [
        "That is about 98%, as computed above, consistent with Chebychev’s bound of “at least 89%”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba60cabd",
      "metadata": {
        "id": "ba60cabd"
      },
      "outputs": [],
      "source": [
        "1-(1/3**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a197dc",
      "metadata": {
        "id": "d6a197dc"
      },
      "outputs": [],
      "source": [
        "united.hist('Delay (Standard Units)', bins=np.arange(-5, 15.5, 0.5))\n",
        "plots.xticks(np.arange(-6, 17, 3));"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "within_45_sd = united.where('Delay (Standard Units)', are.between(-4.5, 4.5))\n",
        "within_45_sd.num_rows/united.num_rows"
      ],
      "metadata": {
        "id": "nUyVTDTL6xpY"
      },
      "id": "nUyVTDTL6xpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "485455f1",
      "metadata": {
        "id": "485455f1"
      },
      "source": [
        "Let us look at the distribution of heights of mothers in our familiar sample of 1,174 mother-newborn pairs. The mothers’ heights have a mean of 64 inches and an SD of 2.5 inches. Unlike the heights of the basketball players, the mothers’ heights are distributed fairly symmetrically about the mean in a bell-shaped curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def29e7",
      "metadata": {
        "id": "9def29e7"
      },
      "outputs": [],
      "source": [
        "baby = Table.read_table('baby.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563f28f5",
      "metadata": {
        "id": "563f28f5"
      },
      "outputs": [],
      "source": [
        "heights = baby.column('Maternal Height')\n",
        "mean_height = np.round(np.mean(heights), 1)\n",
        "mean_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4fa24a",
      "metadata": {
        "id": "8f4fa24a"
      },
      "outputs": [],
      "source": [
        "sd_height = np.round(np.std(heights), 1)\n",
        "sd_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d2b132",
      "metadata": {
        "id": "e8d2b132"
      },
      "outputs": [],
      "source": [
        "baby.hist('Maternal Height')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3922f8",
      "metadata": {
        "id": "fa3922f8"
      },
      "outputs": [],
      "source": [
        "baby.hist('Maternal Height', bins=np.arange(55.5, 72.5, 1), unit='inch')\n",
        "positions = np.arange(-3, 3.1, 1)*sd_height + mean_height\n",
        "plots.xticks(positions);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5bf0b8",
      "metadata": {
        "id": "ee5bf0b8"
      },
      "outputs": [],
      "source": [
        "positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760a928a",
      "metadata": {
        "id": "760a928a"
      },
      "outputs": [],
      "source": [
        "np.arange(-3, 3.1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e882af2c",
      "metadata": {
        "id": "e882af2c"
      },
      "source": [
        "To see how the SD is related to the curve, start at the top of the curve and look towards the right. Notice that there is a place where the curve changes from looking like an “upside-down cup” to a “right-way-up cup”; formally, the curve has a point of inflection. That point is one SD above average. It is the point ***z=1***, which is “average plus 1 SD” = 66.5 inches. Symmetrically on the left-hand side of the mean, the point of inflection is at ***z=-1***, that is, “average minus 1 SD” = 61.5 inches.\n",
        "\n",
        "* For bell-shaped distributions the SD is the distance between the mean and the points of inflection on either side.\n",
        "\n",
        "### Standard Normal Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e04102",
      "metadata": {
        "id": "92e04102"
      },
      "outputs": [],
      "source": [
        "display(Math(r'\\phi(z) = /{\\frac{1}{\\sqrt{2 \\pi}}} e^{-\\frac{1}{2}z^2}, ~~ -\\infty < z < \\infty'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07de246c",
      "metadata": {
        "id": "07de246c"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_and_the_Normal_Curve_11_0.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7391423",
      "metadata": {
        "id": "b7391423"
      },
      "source": [
        "Here are some properties of the curve. Some are apparent by observation, and others require a considerable amount of mathematics to establish.\n",
        "\n",
        "   * The total area under the curve is 1. So you can think of it as a histogram drawn to the density scale.\n",
        "\n",
        "   * The curve is symmetric about 0. So if a variable has this distribution, its mean and median are both 0.\n",
        "\n",
        "   * The points of inflection of the curve are at -1 and +1.\n",
        "\n",
        "   * If a variable has this distribution, its SD is 1. The normal curve is one of the very few distributions that has an SD so clearly identifiable on the histogram.\n",
        "\n",
        "Since we are thinking of the curve as a smoothed histogram, we will want to represent proportions of the total amount of data by areas under the curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2221815e",
      "metadata": {
        "id": "2221815e"
      },
      "outputs": [],
      "source": [
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0bcc684",
      "metadata": {
        "id": "c0bcc684"
      },
      "source": [
        "Let us use this function to find the area to the left of under the standard normal curve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56646fbc",
      "metadata": {
        "id": "56646fbc"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_and_the_Normal_Curve_16_0.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae580a26",
      "metadata": {
        "id": "ae580a26"
      },
      "outputs": [],
      "source": [
        "stats.norm.cdf(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ae6065",
      "metadata": {
        "id": "36ae6065"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_and_the_Normal_Curve_20_0.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea9c69c",
      "metadata": {
        "id": "8ea9c69c"
      },
      "outputs": [],
      "source": [
        "1 - stats.norm.cdf(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab70e93",
      "metadata": {
        "id": "9ab70e93"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_and_the_Normal_Curve_23_0.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8795de24",
      "metadata": {
        "id": "8795de24"
      },
      "outputs": [],
      "source": [
        "stats.norm.cdf(1) - stats.norm.cdf(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0eb765",
      "metadata": {
        "id": "ee0eb765"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_and_the_Normal_Curve_27_0.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60be096",
      "metadata": {
        "id": "e60be096"
      },
      "outputs": [],
      "source": [
        "stats.norm.cdf(2) - stats.norm.cdf(-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9106855b",
      "metadata": {
        "id": "9106855b"
      },
      "outputs": [],
      "source": [
        "stats.norm.cdf(3) - stats.norm.cdf(-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e342a47",
      "metadata": {
        "id": "3e342a47"
      },
      "source": [
        "If a histogram is roughly bell shaped, the proportion of data in the range “average 2 SDs” is about 95%.\n",
        "\n",
        "That is quite a bit more than Chebychev’s lower bound of 75%. Chebychev’s bound is weaker because it has to work for all distributions. If we know that a distribution is normal, we have good approximations to the proportions, not just bounds.\n",
        "\n",
        "The table below compares what we know about all distributions and about normal distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e0e2ec",
      "metadata": {
        "id": "64e0e2ec"
      },
      "source": [
        "<img src=\"https://github.com/Mrsnellek/DS410/blob/main/Week_4/Week_4_Lec/SD_table.png?raw=1\" width=600 height=600 />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b93ed",
      "metadata": {
        "id": "ff9b93ed"
      },
      "source": [
        "### Central Limit Theorem\n",
        "\n",
        "#### Roulette\n",
        "\n",
        "We will play a game of Roulette. A winning bet pays even money, 1 to 1. We define a function red_winnings that returns the net winnings on one $1 bet on red. Specifically, the function takes a color as its argument and returns 1 if the color is red. For all other colors it returns -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acfd2297",
      "metadata": {
        "id": "acfd2297"
      },
      "outputs": [],
      "source": [
        "wheel = Table.read_table('wheel.csv')\n",
        "wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb421807",
      "metadata": {
        "id": "fb421807"
      },
      "outputs": [],
      "source": [
        "def red_winnings(color):\n",
        "    if color == 'red':\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d7c692",
      "metadata": {
        "id": "63d7c692"
      },
      "outputs": [],
      "source": [
        "red = wheel.with_column(\n",
        "    'Winnings: Red', wheel.apply(red_winnings, 'Color')\n",
        "    )\n",
        "red"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772caba1",
      "metadata": {
        "id": "772caba1"
      },
      "source": [
        "Your net gain on one red bet is one random draw from the Winnings: Red column.  You have a 18/38 Change of winning and 20/38 chance of loosing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed04a42",
      "metadata": {
        "id": "7ed04a42"
      },
      "outputs": [],
      "source": [
        "red.select('Winnings: Red').hist(bins=np.arange(-1.5, 1.6, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ef21af",
      "metadata": {
        "id": "17ef21af"
      },
      "source": [
        "Now suppose you bet many times on red. Your net winnings will be the sum of many draws made at random with replacement from the distribution above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690769e8",
      "metadata": {
        "id": "690769e8"
      },
      "outputs": [],
      "source": [
        "num_bets = 400\n",
        "repetitions = 10000\n",
        "\n",
        "net_gain_red = make_array()\n",
        "\n",
        "for i in np.arange(repetitions):\n",
        "    spins = red.sample(num_bets)\n",
        "    new_net_gain_red = spins.column('Winnings: Red').sum()\n",
        "    net_gain_red = np.append(net_gain_red, new_net_gain_red)\n",
        "\n",
        "\n",
        "results = Table().with_column(\n",
        "    'Net Gain on Red', net_gain_red\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba2d3c2",
      "metadata": {
        "id": "bba2d3c2"
      },
      "outputs": [],
      "source": [
        "results.hist(bins=np.arange(-80, 50, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53e8822",
      "metadata": {
        "id": "e53e8822"
      },
      "source": [
        "That’s a roughly bell shaped histogram, even though the distribution we are drawing from is nowhere near bell shaped.\n",
        "\n",
        "The distribution is centered near -20 dollars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df81ea8",
      "metadata": {
        "id": "6df81ea8"
      },
      "outputs": [],
      "source": [
        "average_per_bet = 1*(18/38) + (-1)*(20/38)\n",
        "average_per_bet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b50fc58",
      "metadata": {
        "id": "2b50fc58"
      },
      "outputs": [],
      "source": [
        "400 * average_per_bet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db8d4d",
      "metadata": {
        "id": "42db8d4d"
      },
      "outputs": [],
      "source": [
        "np.mean(results.column(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa39884",
      "metadata": {
        "id": "3fa39884"
      },
      "outputs": [],
      "source": [
        "np.std(results.column(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e37d7bd",
      "metadata": {
        "id": "7e37d7bd"
      },
      "source": [
        "### United Flight Data\n",
        "\n",
        "The table *united* contains data on departure delays of 13,825 United Airlines domestic flights out of San Francisco airport in the summer of 2015. As we have seen before, the distribution of delays has a long right-hand tail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3afa1a9",
      "metadata": {
        "id": "c3afa1a9"
      },
      "outputs": [],
      "source": [
        "united = Table.read_table('united.csv')\n",
        "united"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a069da5",
      "metadata": {
        "id": "8a069da5"
      },
      "outputs": [],
      "source": [
        "united.select('Delay').hist(bins=np.arange(-20, 300, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab283dd",
      "metadata": {
        "id": "aab283dd"
      },
      "outputs": [],
      "source": [
        "mean_delay = np.mean(united.column('Delay'))\n",
        "sd_delay = np.std(united.column('Delay'))\n",
        "\n",
        "mean_delay, sd_delay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14206078",
      "metadata": {
        "id": "14206078"
      },
      "source": [
        "Suppose we sampled 400 delays at random with replacement. We could sample without replacement but the results would be very similar to with-replacement sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470bee1b",
      "metadata": {
        "id": "470bee1b"
      },
      "outputs": [],
      "source": [
        "400/13825*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9f9ad9",
      "metadata": {
        "id": "0e9f9ad9"
      },
      "outputs": [],
      "source": [
        "delay = united.select('Delay')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fee56f8",
      "metadata": {
        "id": "3fee56f8"
      },
      "outputs": [],
      "source": [
        "np.mean(delay.sample(400).column('Delay'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4fbe668",
      "metadata": {
        "id": "e4fbe668"
      },
      "outputs": [],
      "source": [
        "sample_size = 400\n",
        "repetitions = 10000\n",
        "\n",
        "means = make_array()\n",
        "\n",
        "for i in np.arange(repetitions):\n",
        "    sample = delay.sample(sample_size)\n",
        "    new_mean = np.mean(sample.column('Delay'))\n",
        "    means = np.append(means, new_mean)\n",
        "\n",
        "results = Table().with_column(\n",
        "    'Sample Size: 400', means\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4e636c",
      "metadata": {
        "id": "de4e636c"
      },
      "outputs": [],
      "source": [
        "results.hist(bins=np.arange(10, 25, 0.5))\n",
        "plots.scatter(mean_delay, 0, color='red', s=200);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a3ce13",
      "metadata": {
        "id": "71a3ce13"
      },
      "source": [
        "The reason why the bell shape appears in such settings is due to a theory called the ***Central Limit Theorem***.\n",
        "\n",
        "**The Central Limit Theorem says that the probability distribution of the sum or average of a large random sample drawn with replacement will be roughly normal, regardless of the distribution of the population from which the sample is drawn.**\n",
        "\n",
        "As we noted when we were studying Chebychev’s bounds, results that can be applied to random samples regardless of the distribution of the population are very powerful, because in data science we rarely know the distribution of the population.\n",
        "\n",
        "The Central Limit Theorem makes it possible to make inferences with very little knowledge about the population, provided we have a large random sample. That is why it is central to the field of statistical inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3264740d",
      "metadata": {
        "id": "3264740d"
      },
      "source": [
        "How would this distribution change if we increased the sample size? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbec20e",
      "metadata": {
        "id": "4fbec20e"
      },
      "outputs": [],
      "source": [
        "results2 = make_array()\n",
        "\n",
        "sample_size = 1200\n",
        "repetitions= 10000\n",
        "for i in np.arange(repetitions):\n",
        "    sample = delay.sample(sample_size)\n",
        "    new_mean = new_mean = np.mean(sample.column('Delay'))\n",
        "    results2 = np.append(results2, new_mean)\n",
        "    \n",
        "results = results.with_column('Sample  Size: 1200', results2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63acdc9f",
      "metadata": {
        "id": "63acdc9f"
      },
      "outputs": [],
      "source": [
        "results.hist(bins=np.arange(10, 25, 0.5))\n",
        "plots.scatter(mean_delay, 0, color='red', s=200);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671d2f83",
      "metadata": {
        "id": "671d2f83"
      },
      "source": [
        "Both distributions are approximately normal but one is narrower than the other. The proportions based on a sample size of 1200 are more tightly clustered around 16.65 than those from a sample size of 400. Increasing the sample size has decreased the variability in the sample proportion.\n",
        "\n",
        "A larger sample size generally reduces the variability of a statistic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9958af1e",
      "metadata": {
        "id": "9958af1e"
      },
      "source": [
        "In our simulations, we noticed that the means of larger samples tend to be more tightly clustered around the population mean than means of smaller samples. Next, we will quantify the variability of the sample mean and develop a relation between the variability and the sample size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449df8ac",
      "metadata": {
        "id": "449df8ac"
      },
      "source": [
        "Now let’s take random samples and look at the probability distribution of the sample mean. As usual, we will use simulation to get an empirical approximation to this distribution.\n",
        "\n",
        "We will define a function simulate_sample_mean to do this, because we are going to vary the sample size later. The arguments are the name of the table, the label of the column containing the variable, the sample size, and the number of simulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b5e4b48",
      "metadata": {
        "id": "1b5e4b48"
      },
      "outputs": [],
      "source": [
        "\"\"\"Empirical distribution of random sample means\"\"\"\n",
        "\n",
        "def simulate_sample_mean(table, label, sample_size, repetitions):\n",
        "    \n",
        "    means = make_array()\n",
        "\n",
        "    for i in range(repetitions):\n",
        "        new_sample = table.sample(sample_size)\n",
        "        new_sample_mean = np.mean(new_sample.column(label))\n",
        "        means = np.append(means, new_sample_mean)\n",
        "\n",
        "    sample_means = Table().with_column('Sample Means', means)\n",
        "    \n",
        "    # Display empirical histogram and print all relevant quantities\n",
        "    sample_means.hist(bins=20)\n",
        "    plots.xlabel('Sample Means')\n",
        "    plots.title('Sample Size ' + str(sample_size))\n",
        "    print(\"Sample size: \", sample_size)\n",
        "    print(\"Population mean:\", np.mean(table.column(label)))\n",
        "    print(\"Average of sample means: \", np.mean(means))\n",
        "    print(\"Population SD:\", np.std(table.column(label)))\n",
        "    print(\"SD of sample means:\", np.std(means))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33e7f90",
      "metadata": {
        "id": "c33e7f90"
      },
      "outputs": [],
      "source": [
        "simulate_sample_mean(delay, 'Delay', 100, 10000)\n",
        "plots.xlim(5, 35)\n",
        "plots.ylim(0, 0.25);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd294084",
      "metadata": {
        "id": "bd294084"
      },
      "outputs": [],
      "source": [
        "simulate_sample_mean(delay, 'Delay', 400, 10000)\n",
        "plots.xlim(5, 35)\n",
        "plots.ylim(0, 0.25);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0de0ae",
      "metadata": {
        "id": "8e0de0ae"
      },
      "outputs": [],
      "source": [
        "simulate_sample_mean(delay, 'Delay', 625, 10000)\n",
        "plots.xlim(5, 35)\n",
        "plots.ylim(0, 0.25);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083f436f",
      "metadata": {
        "id": "083f436f"
      },
      "source": [
        "You can see the Central Limit Theorem in action – the histograms of the sample means are roughly normal, even though the histogram of the delays themselves is far from normal.\n",
        "\n",
        "You can also see that each of the three histograms of the sample means is centered very close to the population mean. In each case, the “average of sample means” is very close to 16.66 minutes, the population mean. Both values are provided in the printout above each histogram. As expected, the sample mean is an unbiased estimate of the population mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffcf4128",
      "metadata": {
        "id": "ffcf4128"
      },
      "outputs": [],
      "source": [
        "pop_sd = np.std(delay.column('Delay'))\n",
        "pop_sd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f300e20",
      "metadata": {
        "id": "2f300e20"
      },
      "source": [
        "Let's calcuate and table the SD of 10,000 sample means  from sample sizes in np.arange(25, 626, 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97946a4",
      "metadata": {
        "id": "c97946a4"
      },
      "outputs": [],
      "source": [
        "repetitions = 10000\n",
        "sample_sizes = np.arange(25, 626, 25)\n",
        "\n",
        "sd_means = make_array()\n",
        "\n",
        "for n in sample_sizes:\n",
        "    means = make_array()\n",
        "    for i in np.arange(repetitions):\n",
        "        means = np.append(means, np.mean(delay.sample(n).column('Delay')))\n",
        "    sd_means = np.append(sd_means, np.std(means))\n",
        "\n",
        "sd_comparison = Table().with_columns(\n",
        "    'Sample Size n', sample_sizes,\n",
        "    'SD of 10,000 Sample Means', sd_means,\n",
        "    'pop_sd/sqrt(n)', pop_sd/np.sqrt(sample_sizes)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96088c7d",
      "metadata": {
        "id": "96088c7d"
      },
      "outputs": [],
      "source": [
        "sd_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8d03a1a",
      "metadata": {
        "id": "d8d03a1a"
      },
      "outputs": [],
      "source": [
        "sd_comparison.plot('Sample Size n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eadac832",
      "metadata": {
        "id": "eadac832"
      },
      "source": [
        "### Central Limit Theorem for the Sample Mean\n",
        "\n",
        "If you draw a large random sample with replacement from a population, then, regardless of the distribution of the population, the probability distribution of the sample mean is roughly normal, centered at the population mean, with an SD equal to the population SD divided by the square root of the sample size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9f4f20",
      "metadata": {
        "id": "0c9f4f20"
      },
      "source": [
        "If you draw a large random sample with replacement from a population, then, regardless of the distribution of the population, the probability distribution of the sample mean is roughly normal, centered at the population mean, with an SD equal to the population SD divided by the square root of the sample size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4854e119",
      "metadata": {
        "id": "4854e119"
      },
      "source": [
        "$$ {\\mbox{SD of all possible sample means}} ~=~\n",
        "\\frac{\\mbox{Population SD}}{\\sqrt{\\mbox{sample size}}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5feda0ba",
      "metadata": {
        "id": "5feda0ba"
      },
      "source": [
        "\n",
        "\n",
        "   * The population size doesn’t affect the accuracy of the sample mean. The population size doesn’t appear anywhere in the formula.\n",
        "\n",
        "   * The population SD is a constant; it’s the same for every sample drawn from the population. The sample size can be varied. Because the sample size appears in the denominator, the variability of the sample mean decreases as the sample size increases, and hence the accuracy increases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6758b2",
      "metadata": {
        "id": "ef6758b2"
      },
      "source": [
        "* In general, when you multiply the sample size by a factor, the accuracy of the sample mean goes up by the square root of that factor.\n",
        "\n",
        "* So to increase accuracy by a factor of 10, you have to multiply sample size by a factor of 100. Accuracy doesn’t come cheap!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229084c3",
      "metadata": {
        "id": "229084c3"
      },
      "source": [
        "## The Sample Size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5db89eb",
      "metadata": {
        "id": "a5db89eb"
      },
      "source": [
        "Candidate A is contesting an election. A polling organization wants to estimate the proportion of voters who will vote for her. Let’s suppose that they plan to take a simple random sample of voters, though in reality their method of sampling would be more complex. How can they decide how large their sample should be, to get a desired level of accuracy?\n",
        "\n",
        "We are now in a position to answer this question, after making a few assumptions:\n",
        "\n",
        "   * The population of voters is very large and that therefore we can just as well assume that the random sample will be drawn with replacement.\n",
        "\n",
        "   * The polling organization will make its estimate by constructing an approximate 95% confidence interval for the percent of voters who will vote for Candidate A.\n",
        "\n",
        "   * The desired level of accuracy is that the width of the interval should be no more than 1%. That’s pretty accurate! For example, the confidence interval (33.2%, 34%) would be fine but (33.2%, 35%) would not.\n",
        "\n",
        "We will work with the sample proportion of voters for Candidate A. Recall that a proportion is a mean, when the values in the population are only 0 (the type of individual you are not counting) or 1 (the type of individual you are counting)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c5813b",
      "metadata": {
        "id": "b0c5813b"
      },
      "source": [
        "If we had a random sample, we could go about using the bootstrap to construct a confidence interval for the percent of voters for Candidate A. But we don’t have a sample yet – we are trying to find out how big the sample has to be so that our confidence interval is as narrow as we want it to be."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ac84db",
      "metadata": {
        "id": "19ac84db"
      },
      "source": [
        "We know that for normally distributed variables, the interval “center +- 2 SDs” contains 95% of the data.\n",
        "\n",
        "The confidence interval will stretch for 2 SDs of the sample proportion, on either side of the center. So the width of the interval will be 4 SDs of the sample proportion.\n",
        "\n",
        "We are willing to tolerate a width of 1% = 0.01. So, using the formula developed in the last section:\n",
        "\n",
        "\n",
        "$$\n",
        "4 \\times \\frac{\\mbox{SD of the 0-1 population}}{\\sqrt{\\mbox{sample size}}} ~ \\le ~ 0.01 $$\n",
        "\n",
        "So \n",
        "\n",
        "$$ \\sqrt{\\mbox{sample size}} ~ \\ge ~ 4 \\times \\frac{\\mbox{SD of the 0-1 population}}{0.01} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e009269",
      "metadata": {
        "id": "0e009269"
      },
      "source": [
        "If we knew the SD of the population, we’d be done. We could calculate the square root of the sample size, and then take the square to get the sample size. But we don’t know the SD of the population. The population consists of 1 for each voter for Candidate A, and 0 for all other voters, and we don’t know what proportion of each kind there are. That’s what we’re trying to estimate.\n",
        "\n",
        "So are we stuck? No, because we can bound the SD of the population. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76451b0c",
      "metadata": {
        "id": "76451b0c"
      },
      "source": [
        "Let’s calculate the SDs of populations of 10 elements that only consist of 0’s and 1’s, in varying proportions. The function np.ones is useful for this. It takes a positive integer as its argument and returns an array consisting of that many 1’s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe0eeb8",
      "metadata": {
        "id": "8fe0eeb8"
      },
      "outputs": [],
      "source": [
        "sd = make_array()\n",
        "for i in np.arange(1, 10, 1):\n",
        "    # Create an array of i 1's and (10-i) 0's\n",
        "    population = np.append(np.ones(i), 1-np.ones(10-i))\n",
        "    sd = np.append(sd, np.std(population))\n",
        "    \n",
        "zero_one_sds = Table().with_columns(\n",
        "    \"Population Proportion of 1's\", np.arange(0.1, 1, 0.1),\n",
        "    \"Population SD\", sd\n",
        ")\n",
        "\n",
        "zero_one_sds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0841a03",
      "metadata": {
        "id": "b0841a03"
      },
      "source": [
        "Not surprisingly, the SD of a population with 10% 1’s and 90% 0’s is the same as that of a population with 90% 1’s and 10% 0’s.  More importantly for our purposes, the SD increases as the proportion of 1’s increases, until the proportion of 1’s is 0.5; then it starts to decrease symmetrically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e9a67e",
      "metadata": {
        "id": "22e9a67e"
      },
      "outputs": [],
      "source": [
        "zero_one_sds.scatter(\"Population Proportion of 1's\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db32dcdb",
      "metadata": {
        "id": "db32dcdb"
      },
      "source": [
        "**Summary:** The SD of a population of 1’s and 0’s is at most 0.5. That’s the value of the SD when 50% of the population is coded 1 and the other 50% are coded 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9051264c",
      "metadata": {
        "id": "9051264c"
      },
      "source": [
        "We know that:\n",
        "\n",
        "$$ \\sqrt{\\mbox{sample size}} ~ \\ge ~ 4 \\times \\frac{\\mbox{SD of the 0-1 population}}{0.01}  $$\n",
        "\n",
        "And that the SD of the 0-1 population is at most 0.5, regardless of the proportion of 1’s in the population. So it is safe to take\n",
        "\n",
        "$$ \\sqrt{\\mbox{sample size}} ~ \\ge ~ 4 \\times \\frac{0.5}{0.01} ~=~ 200 $$ \n",
        "\n",
        "So the sample size should be at least $200^2 = 40,000$. That’s an enormous sample! But that’s what you need if you want to guarantee great accuracy with high confidence no matter what the population looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c6260f",
      "metadata": {
        "id": "f4c6260f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}